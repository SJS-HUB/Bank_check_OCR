{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e134e9-365a-453e-a6e2-a9e57bcb22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "!pip install google-cloud-vision openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a3232-66c2-451b-a40d-7f531470573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Import and verify core libraries\n",
    "import sys\n",
    "import pkg_resources\n",
    "import importlib\n",
    "\n",
    "# Test essential libraries\n",
    "required_packages = [\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'scikit-learn',\n",
    "    'tensorflow',  # or 'torch' if using PyTorch\n",
    "    'pytesseract',\n",
    "    'easyocr'\n",
    "]\n",
    "\n",
    "print(\"Checking required packages...\")\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'opencv-python':\n",
    "            import cv2\n",
    "            print(f\"âœ“ OpenCV: {cv2.__version__}\")\n",
    "        elif package == 'pillow':\n",
    "            import PIL\n",
    "            print(f\"âœ“ Pillow: {PIL.__version__}\")\n",
    "        elif package == 'numpy':\n",
    "            import numpy as np\n",
    "            print(f\"âœ“ NumPy: {np.__version__}\")\n",
    "        elif package == 'pandas':\n",
    "            import pandas as pd\n",
    "            print(f\"âœ“ Pandas: {pd.__version__}\")\n",
    "        elif package == 'matplotlib':\n",
    "            import matplotlib\n",
    "            print(f\"âœ“ Matplotlib: {matplotlib.__version__}\")\n",
    "        elif package == 'scikit-learn':\n",
    "            import sklearn\n",
    "            print(f\"âœ“ Scikit-learn: {sklearn.__version__}\")\n",
    "        elif package == 'tensorflow':\n",
    "            import tensorflow as tf\n",
    "            print(f\"âœ“ TensorFlow: {tf.__version__}\")\n",
    "        elif package == 'pytesseract':\n",
    "            import pytesseract\n",
    "            print(f\"âœ“ PyTesseract: Available\")\n",
    "        elif package == 'easyocr':\n",
    "            import easyocr\n",
    "            print(f\"âœ“ EasyOCR: Available\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"âœ— {package}: Not installed\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nMissing packages: {missing_packages}\")\n",
    "    print(\"Please install using: pip install\", \" \".join(missing_packages))\n",
    "else:\n",
    "    print(\"\\nâœ“ All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fec822-2c9b-40d3-a512-8cf0c65cbe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2a: Install OpenCV\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"âœ“ Successfully installed {package_name}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âœ— Failed to install {package_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Install OpenCV first\n",
    "print(\"Installing OpenCV...\")\n",
    "install_package(\"opencv-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e55a72-c948-42a4-b122-7796898278ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2b: Install TensorFlow\n",
    "print(\"Installing TensorFlow...\")\n",
    "install_package(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125903-dd2c-43ec-85b2-de3186294b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2c: Install OCR libraries\n",
    "print(\"Installing PyTesseract...\")\n",
    "install_package(\"pytesseract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e22e3-65a1-40f9-9e18-adf03a052940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2d: Install EasyOCR\n",
    "print(\"Installing EasyOCR...\")\n",
    "install_package(\"easyocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11956051-f153-436c-9be5-50516932ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: Final verification of all installations\n",
    "print(\"Verifying all installations...\")\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"âœ“ OpenCV: {cv2.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— OpenCV: {e}\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"âœ“ TensorFlow: {tf.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— TensorFlow: {e}\")\n",
    "\n",
    "try:\n",
    "    import pytesseract\n",
    "    print(f\"âœ“ PyTesseract: Available\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— PyTesseract: {e}\")\n",
    "\n",
    "try:\n",
    "    import easyocr\n",
    "    print(f\"âœ“ EasyOCR: Available\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— EasyOCR: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYBRID OCR SYSTEM READY!\")\n",
    "print(\"Local OCR: EasyOCR + PyTesseract\")\n",
    "print(\"Cloud OCR: Google Vision API\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27caafa1-2a42-4fde-9090-7e1f1b5a1ee7",
   "metadata": {},
   "source": [
    "## Step 2: Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec57b9b-5ada-400f-b542-867b6fb3c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Create project directory structure\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define project structure\n",
    "project_structure = {\n",
    "    'data': {\n",
    "        'raw': 'Original check images',\n",
    "        'processed': 'Preprocessed images',\n",
    "        'train': 'Training dataset',\n",
    "        'test': 'Test dataset',\n",
    "        'validation': 'Validation dataset'\n",
    "    },\n",
    "    'models': 'Trained models storage',\n",
    "    'outputs': {\n",
    "        'results': 'OCR results',\n",
    "        'logs': 'Processing logs',\n",
    "        'reports': 'Analysis reports'\n",
    "    },\n",
    "    'config': 'Configuration files',\n",
    "    'utils': 'Utility functions'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "base_dir = Path('bank_check_ocr')\n",
    "print(\"Creating project structure...\")\n",
    "\n",
    "def create_directories(structure, parent_path=base_dir):\n",
    "    for key, value in structure.items():\n",
    "        current_path = parent_path / key\n",
    "        current_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            create_directories(value, current_path)\n",
    "        else:\n",
    "            # Create a README file in each directory\n",
    "            readme_path = current_path / 'README.md'\n",
    "            if not readme_path.exists():\n",
    "                readme_path.write_text(f\"# {key.title()} Directory\\n\\n{value}\\n\")\n",
    "        \n",
    "        print(f\"âœ“ Created: {current_path}\")\n",
    "\n",
    "create_directories(project_structure)\n",
    "print(f\"\\nâœ“ Project structure created successfully!\")\n",
    "print(f\"Base directory: {base_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44180390-9fa8-4b39-bed9-5a976ad620a4",
   "metadata": {},
   "source": [
    "## Step 2.2: Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55707f2c-9585-4d31-b7c7-ddc030517526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Create configuration file for hybrid OCR system\n",
    "config = {\n",
    "    \"ocr_settings\": {\n",
    "        \"local_ocr\": {\n",
    "            \"easyocr\": {\n",
    "                \"languages\": [\"en\"],\n",
    "                \"gpu\": False,\n",
    "                \"confidence_threshold\": 0.5\n",
    "            },\n",
    "            \"tesseract\": {\n",
    "                \"config\": \"--oem 3 --psm 6\",\n",
    "                \"confidence_threshold\": 30\n",
    "            }\n",
    "        },\n",
    "        \"google_vision\": {\n",
    "            \"api_key_path\": \"config/google_vision_api_key.json\",\n",
    "            \"features\": [\"TEXT_DETECTION\", \"DOCUMENT_TEXT_DETECTION\"],\n",
    "            \"confidence_threshold\": 0.8\n",
    "        }\n",
    "    },\n",
    "    \"image_processing\": {\n",
    "        \"resize_width\": 1200,\n",
    "        \"resize_height\": 800,\n",
    "        \"dpi\": 300,\n",
    "        \"preprocessing_steps\": [\n",
    "            \"grayscale\",\n",
    "            \"noise_reduction\",\n",
    "            \"contrast_enhancement\",\n",
    "            \"deskew\"\n",
    "        ]\n",
    "    },\n",
    "    \"hybrid_logic\": {\n",
    "        \"use_local_first\": True,\n",
    "        \"fallback_to_cloud\": True,\n",
    "        \"confidence_comparison\": True,\n",
    "        \"cost_optimization\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = base_dir / 'config' / 'ocr_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"âœ“ Configuration file created!\")\n",
    "print(f\"Config saved to: {config_path}\")\n",
    "print(\"\\nConfiguration preview:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34def61c-fc53-4525-8cdf-ef01c9d90769",
   "metadata": {},
   "source": [
    "# Step 3: Initialize Hybrid OCR System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da3825-19f6-4c89-9197-802c97bcd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Initialize OCR engines\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Load configuration\n",
    "with open('bank_check_ocr/config/ocr_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Initializing OCR engines...\")\n",
    "\n",
    "# Initialize EasyOCR (this will download models on first run)\n",
    "print(\"Loading EasyOCR...\")\n",
    "try:\n",
    "    reader = easyocr.Reader(['en'], gpu=False)\n",
    "    print(\"âœ“ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— EasyOCR initialization failed: {e}\")\n",
    "\n",
    "# Test PyTesseract\n",
    "print(\"Testing PyTesseract...\")\n",
    "try:\n",
    "    # Create a simple test image\n",
    "    test_img = np.ones((100, 300, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(test_img, \"TEST OCR\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    \n",
    "    # Test tesseract\n",
    "    pil_img = Image.fromarray(test_img)\n",
    "    test_result = pytesseract.image_to_string(pil_img)\n",
    "    print(f\"âœ“ PyTesseract working - Test result: '{test_result.strip()}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— PyTesseract test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OCR ENGINES INITIALIZED!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5603f03-54df-491f-ba27-8d873ff61cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1b: Install Tesseract executable\n",
    "print(\"Installing Tesseract executable...\")\n",
    "\n",
    "# For Windows users (most common case)\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "system = platform.system()\n",
    "print(f\"Detected system: {system}\")\n",
    "\n",
    "if system == \"Windows\":\n",
    "    print(\"Installing Tesseract for Windows...\")\n",
    "    try:\n",
    "        # Install using conda-forge (most reliable for Windows)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tesseract\"])\n",
    "        print(\"âœ“ Tesseract package installed\")\n",
    "    except:\n",
    "        print(\"Pip install failed, trying alternative...\")\n",
    "        \n",
    "    # Alternative: Download and install instructions\n",
    "    print(\"\\nIf installation fails, please:\")\n",
    "    print(\"1. Download Tesseract from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "    print(\"2. Install it to C:\\\\Program Files\\\\Tesseract-OCR\")\n",
    "    print(\"3. Add C:\\\\Program Files\\\\Tesseract-OCR to your PATH\")\n",
    "    \n",
    "elif system == \"Linux\":\n",
    "    print(\"For Linux, run: sudo apt-get install tesseract-ocr\")\n",
    "    \n",
    "elif system == \"Darwin\":  # macOS\n",
    "    print(\"For macOS, run: brew install tesseract\")\n",
    "\n",
    "print(\"\\nLet's try a different approach - using only EasyOCR for now...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0e265-b66e-4498-9c3c-0510a52ffac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Create Hybrid OCR class (EasyOCR focused)\n",
    "class HybridOCR:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.easyocr_reader = reader  # Use the reader we initialized above\n",
    "        self.results_log = []\n",
    "        self.tesseract_available = False\n",
    "        \n",
    "        # Test if Tesseract is available\n",
    "        try:\n",
    "            test_img = np.ones((50, 200, 3), dtype=np.uint8) * 255\n",
    "            cv2.putText(test_img, \"TEST\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "            pil_img = Image.fromarray(test_img)\n",
    "            pytesseract.image_to_string(pil_img)\n",
    "            self.tesseract_available = True\n",
    "            print(\"âœ“ Tesseract is available\")\n",
    "        except:\n",
    "            print(\"âš  Tesseract not available - using EasyOCR only\")\n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocess image for better OCR results\"\"\"\n",
    "        try:\n",
    "            # Read image\n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Resize if needed\n",
    "            height, width = gray.shape\n",
    "            if width > self.config['image_processing']['resize_width']:\n",
    "                scale = self.config['image_processing']['resize_width'] / width\n",
    "                new_width = int(width * scale)\n",
    "                new_height = int(height * scale)\n",
    "                gray = cv2.resize(gray, (new_width, new_height))\n",
    "            \n",
    "            # Noise reduction\n",
    "            denoised = cv2.medianBlur(gray, 3)\n",
    "            \n",
    "            # Contrast enhancement\n",
    "            enhanced = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(denoised)\n",
    "            \n",
    "            return enhanced\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Preprocessing error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def easyocr_extract(self, image):\n",
    "        \"\"\"Extract text using EasyOCR\"\"\"\n",
    "        try:\n",
    "            results = self.easyocr_reader.readtext(image)\n",
    "            \n",
    "            extracted_data = []\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence >= self.config['ocr_settings']['local_ocr']['easyocr']['confidence_threshold']:\n",
    "                    extracted_data.append({\n",
    "                        'text': text,\n",
    "                        'confidence': confidence,\n",
    "                        'bbox': bbox,\n",
    "                        'engine': 'easyocr'\n",
    "                    })\n",
    "            \n",
    "            return extracted_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"EasyOCR extraction error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        print(f\"Processing: {image_path}\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_img = self.preprocess_image(image_path)\n",
    "        if processed_img is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract text using EasyOCR\n",
    "        results = self.easyocr_extract(processed_img)\n",
    "        \n",
    "        # Log results\n",
    "        self.results_log.append({\n",
    "            'image_path': str(image_path),\n",
    "            'timestamp': time.time(),\n",
    "            'results': results,\n",
    "            'engine_used': 'easyocr'\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize our hybrid OCR system\n",
    "hybrid_ocr = HybridOCR(config)\n",
    "print(\"âœ“ Hybrid OCR class created successfully!\")\n",
    "print(f\"Tesseract available: {hybrid_ocr.tesseract_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dffcf-b7fa-4298-9c25-5027e9a95bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Create sample test images that simulate bank check text\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def create_test_images():\n",
    "    \"\"\"Create sample images that simulate bank check elements\"\"\"\n",
    "    \n",
    "    # Create test images directory\n",
    "    test_dir = Path('bank_check_ocr/data/test')\n",
    "    test_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Test Image 1: Simple bank routing/account numbers\n",
    "    img1 = np.ones((200, 600, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(img1, \"ROUTING: 021000021\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.putText(img1, \"ACCOUNT: 1234567890\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.putText(img1, \"CHECK #: 001\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.imwrite(str(test_dir / 'test_check_1.png'), img1)\n",
    "    \n",
    "    # Test Image 2: Amount and date\n",
    "    img2 = np.ones((200, 600, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(img2, \"DATE: 12/25/2024\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.putText(img2, \"AMOUNT: $1,250.00\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.putText(img2, \"PAY TO: JOHN SMITH\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    cv2.imwrite(str(test_dir / 'test_check_2.png'), img2)\n",
    "    \n",
    "    # Test Image 3: Mixed fonts and sizes\n",
    "    img3 = np.ones((250, 700, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(img3, \"BANK OF AMERICA\", (50, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 2)\n",
    "    cv2.putText(img3, \"123 Main Street\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "    cv2.putText(img3, \"New York, NY 10001\", (50, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "    cv2.putText(img3, \"MEMO: Monthly Rent\", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    cv2.imwrite(str(test_dir / 'test_check_3.png'), img3)\n",
    "    \n",
    "    print(\"âœ“ Created 3 test images\")\n",
    "    return [\n",
    "        test_dir / 'test_check_1.png',\n",
    "        test_dir / 'test_check_2.png',\n",
    "        test_dir / 'test_check_3.png'\n",
    "    ]\n",
    "\n",
    "# Create test images\n",
    "test_images = create_test_images()\n",
    "\n",
    "# Display one test image to verify\n",
    "plt.figure(figsize=(10, 4))\n",
    "sample_img = cv2.imread(str(test_images[0]))\n",
    "sample_img_rgb = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(sample_img_rgb)\n",
    "plt.title(\"Sample Test Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test images created: {len(test_images)}\")\n",
    "for img_path in test_images:\n",
    "    print(f\"  - {img_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522391bd-007d-4d37-9d65-6c34336cd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Test OCR on our sample images\n",
    "def test_ocr_system():\n",
    "    \"\"\"Test OCR system on sample images\"\"\"\n",
    "    \n",
    "    print(\"Testing OCR system on sample images...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, img_path in enumerate(test_images, 1):\n",
    "        print(f\"\\nTest {i}: {img_path.name}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Process image\n",
    "        results = hybrid_ocr.process_image(img_path)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"Found {len(results)} text elements:\")\n",
    "            for j, result in enumerate(results, 1):\n",
    "                print(f\"  {j}. Text: '{result['text']}'\")\n",
    "                print(f\"     Confidence: {result['confidence']:.2f}\")\n",
    "                print(f\"     Engine: {result['engine']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No text detected!\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nOCR Test Complete!\")\n",
    "    print(f\"Total images processed: {len(hybrid_ocr.results_log)}\")\n",
    "\n",
    "# Run the test\n",
    "test_ocr_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87532d-8769-43f9-b944-dd4876b87684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Extract your bank cheque dataset\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_dataset():\n",
    "    \"\"\"Extract the bank cheque dataset\"\"\"\n",
    "    \n",
    "    # Look for the zip file\n",
    "    zip_path = Path('bank_cheque_images_dataset.zip')\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(\"Dataset zip file not found. Let's check what files are available:\")\n",
    "        current_dir = Path('.')\n",
    "        for file in current_dir.iterdir():\n",
    "            if file.suffix == '.zip':\n",
    "                print(f\"Found zip file: {file.name}\")\n",
    "                zip_path = file\n",
    "                break\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(\"No zip file found. Please make sure the dataset is uploaded.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found dataset: {zip_path.name}\")\n",
    "    print(f\"Size: {zip_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Create extraction directory\n",
    "    extract_dir = Path('bank_check_ocr/data/raw')\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            print(\"Extracting dataset...\")\n",
    "            zip_ref.extractall(extract_dir)\n",
    "            print(f\"âœ“ Dataset extracted to: {extract_dir}\")\n",
    "            \n",
    "            # List contents\n",
    "            print(\"\\nDataset contents:\")\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"Total files: {len(file_list)}\")\n",
    "            \n",
    "            # Show first few files\n",
    "            for i, file_name in enumerate(file_list[:10]):\n",
    "                print(f\"  {i+1}. {file_name}\")\n",
    "            \n",
    "            if len(file_list) > 10:\n",
    "                print(f\"  ... and {len(file_list) - 10} more files\")\n",
    "                \n",
    "        return extract_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the dataset\n",
    "dataset_path = extract_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c91f7-8da5-4571-8e81-0c21232c112b",
   "metadata": {},
   "source": [
    "## Step 5.2: Explore the Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b208b8-eb8a-45a5-9dc5-208796b8c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Explore the extracted dataset\n",
    "def explore_dataset(dataset_path):\n",
    "    \"\"\"Explore the structure of the extracted dataset\"\"\"\n",
    "    \n",
    "    if dataset_path is None:\n",
    "        print(\"No dataset path provided\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Exploring dataset at: {dataset_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(list(dataset_path.rglob(f'*{ext}')))\n",
    "        image_files.extend(list(dataset_path.rglob(f'*{ext.upper()}')))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    \n",
    "    if image_files:\n",
    "        print(\"\\nFirst 5 image files:\")\n",
    "        for i, img_path in enumerate(image_files[:5]):\n",
    "            relative_path = img_path.relative_to(dataset_path)\n",
    "            file_size = img_path.stat().st_size / 1024  # KB\n",
    "            print(f\"  {i+1}. {relative_path} ({file_size:.1f} KB)\")\n",
    "        \n",
    "        # Check directory structure\n",
    "        print(\"\\nDirectory structure:\")\n",
    "        directories = set()\n",
    "        for img_path in image_files:\n",
    "            relative_path = img_path.relative_to(dataset_path)\n",
    "            if relative_path.parent != Path('.'):\n",
    "                directories.add(str(relative_path.parent))\n",
    "        \n",
    "        if directories:\n",
    "            for directory in sorted(directories):\n",
    "                count = len([f for f in image_files if directory in str(f.relative_to(dataset_path))])\n",
    "                print(f\"  {directory}: {count} files\")\n",
    "        else:\n",
    "            print(\"  All files in root directory\")\n",
    "    \n",
    "    return image_files\n",
    "\n",
    "# Explore the dataset\n",
    "if dataset_path:\n",
    "    image_files = explore_dataset(dataset_path)\n",
    "else:\n",
    "    print(\"Dataset extraction failed. Please check the zip file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353fa6c-a8cc-489e-81a9-227bf8c03879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3: Load and display sample cheque images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample_images(image_files, num_samples=4):\n",
    "    \"\"\"Display sample images from the dataset\"\"\"\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No image files found!\")\n",
    "        return\n",
    "    \n",
    "    # Select sample images\n",
    "    sample_images = image_files[:num_samples]\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    print(\"Loading sample cheque images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(sample_images):\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                # Convert BGR to RGB for matplotlib\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Display image\n",
    "                axes[i].imshow(img_rgb)\n",
    "                axes[i].set_title(f\"{img_path.name}\\n{img.shape[1]}x{img.shape[0]}\")\n",
    "                axes[i].axis('off')\n",
    "                \n",
    "                print(f\"âœ“ Loaded {img_path.name}: {img.shape}\")\n",
    "            else:\n",
    "                print(f\"âœ— Failed to load {img_path.name}\")\n",
    "                axes[i].text(0.5, 0.5, f\"Failed to load\\n{img_path.name}\", \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].axis('off')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error loading {img_path.name}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f\"Error loading\\n{img_path.name}\", \n",
    "                       ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sample_images\n",
    "\n",
    "# Display sample images\n",
    "if 'image_files' in locals() and image_files:\n",
    "    sample_images = display_sample_images(image_files)\n",
    "else:\n",
    "    print(\"Please run the dataset exploration code first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f00399-87be-4d92-80dd-51f94d179bb8",
   "metadata": {},
   "source": [
    "## Step 5: Basic OCR Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60758e3d-633a-480a-af41-49908ceb9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OCR on your check images\n",
    "def test_ocr_on_checks(image_files, num_tests=3):\n",
    "    \"\"\"Test OCR on actual check images\"\"\"\n",
    "    \n",
    "    test_images = image_files[:num_tests]\n",
    "    \n",
    "    for i, img_path in enumerate(test_images):\n",
    "        print(f\"\\n=== Testing {img_path.name} ===\")\n",
    "        \n",
    "        results = hybrid_ocr.process_image(img_path)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"Detected {len(results)} text elements:\")\n",
    "            for j, result in enumerate(results, 1):\n",
    "                print(f\"{j}. '{result['text']}' (confidence: {result['confidence']:.2f})\")\n",
    "        else:\n",
    "            print(\"No text detected\")\n",
    "\n",
    "# Run test\n",
    "test_ocr_on_checks(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204099dc-26d8-457c-81f9-b359513af484",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8492511-e67b-4c5e-8c72-b7353e6ccdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced preprocessing for better OCR\n",
    "def advanced_preprocess(image_path):\n",
    "    \"\"\"Advanced preprocessing for check images\"\"\"\n",
    "    \n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Morphological operations\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Test advanced preprocessing\n",
    "test_img = advanced_preprocess(image_files[0])\n",
    "if test_img is not None:\n",
    "    results = hybrid_ocr.easyocr_reader.readtext(test_img)\n",
    "    print(f\"Advanced preprocessing results: {len(results)} text elements found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d71e74-807f-4e4e-ae7b-281fbf0facb2",
   "metadata": {},
   "source": [
    "## Step 7: Install Google Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa91f53-87ba-4a49-9cb7-92de7d98fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Google Cloud Vision\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-vision\"])\n",
    "    print(\"âœ“ Google Vision API installed\")\n",
    "except Exception as e:\n",
    "    print(f\"Installation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926822f3-524f-4f52-8b08-dcccc28d774f",
   "metadata": {},
   "source": [
    "## Step 8: Test Google Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edc4f9a-f4de-43f4-a453-90f1cd9afa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google Vision API client created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test Google Vision API with your key\n",
    "def test_google_vision_setup():\n",
    "    \"\"\"Test Google Vision API setup\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from google.cloud import vision\n",
    "        import os\n",
    "        \n",
    "        # Set your API key path\n",
    "        key_path = \"VisionApiKey.json\"  # Your uploaded key file\n",
    "        \n",
    "        # Check if key file exists\n",
    "        if not os.path.exists(key_path):\n",
    "            print(f\" API key file not found: {key_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Set environment variable\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n",
    "        \n",
    "        # Test API connection\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        print(\" Google Vision API client created successfully!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Google Vision setup error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the setup\n",
    "google_vision_ready = test_google_vision_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c145bad-c0df-49c1-9167-95d685ed1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "import os\n",
    "\n",
    "# ğŸ› ï¸ Set your API key path BEFORE running the loop\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"VisionApiKey.json\"\n",
    "\n",
    "def extract_text_google_vision(image_path):\n",
    "    \"\"\"\n",
    "    Extracts text using Google Cloud Vision API from a given image.\n",
    "    Returns a list of dictionaries with text and bounding box info.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "\n",
    "        image = vision.Image(content=content)\n",
    "        response = client.text_detection(image=image)\n",
    "\n",
    "        results = []\n",
    "        if response.text_annotations:\n",
    "            for annotation in response.text_annotations:\n",
    "                text = annotation.description\n",
    "                vertices = annotation.bounding_poly.vertices\n",
    "                bbox = [(v.x, v.y) for v in vertices]\n",
    "\n",
    "                results.append({\n",
    "                    'text': text,\n",
    "                    'bbox': bbox,\n",
    "                    'engine': 'google_vision'\n",
    "                })\n",
    "\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting text from {image_path}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442ba15-d716-49b5-b667-684be24f8f63",
   "metadata": {},
   "source": [
    "## Process All Images in a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba5cc224-44ff-4f91-a1f0-1ecfb48b7350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Folder already exists: cheques_india\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"IDRBT_Cheque_Image_Dataset.zip\"\n",
    "extract_path = \"cheques_india\"\n",
    "\n",
    "# Unzip if not already done\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "        print(f\"âœ“ Extracted ZIP to folder: {extract_path}\")\n",
    "else:\n",
    "    print(f\"âœ“ Folder already exists: {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89db2a06-e30b-45c0-a4e7-a40f24691667",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = extract_path  # âœ… Now this is the actual folder\n",
    "output_records = []\n",
    "\n",
    "for idx, filename in enumerate(os.listdir(image_folder)):\n",
    "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        print(f\"[{idx+1}] Processing: {filename}\")\n",
    "        \n",
    "        ocr_results = extract_text_google_vision(image_path)\n",
    "\n",
    "        for item in ocr_results:\n",
    "            output_records.append({\n",
    "                \"image\": filename,\n",
    "                \"text\": item['text'],\n",
    "                \"bbox\": item['bbox'],\n",
    "                \"engine\": item['engine']\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01f60e6d-0192-4fe4-a417-e7629f854407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OCR results saved to cheque_ocr_output1.xlsx\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(output_records)\n",
    "df.to_excel(\"cheque_ocr_output2.xlsx\", index=False)\n",
    "print(\"âœ… OCR results saved to cheque_ocr_output1.xlsx\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5ff7fc8-c753-48d5-a158-36ac2163eec4",
   "metadata": {},
   "source": [
    "your_project/\n",
    "â”œâ”€â”€ VisionApiKey.json\n",
    "â”œâ”€â”€ bank_cheqque_images_dataset.zip\n",
    "â”œâ”€â”€ cheques/  â† extracted images go here\n",
    "â”œâ”€â”€ your_notebook.ipynb\n",
    "â”œâ”€â”€ cheque_ocr_output.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aa0c7ca-f9cd-44a9-ad33-e638af017f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_folder = \"cheques_india\"  # or whatever your extracted folder is\n",
    "output_records = []\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "for root, dirs, files in os.walk(image_folder):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            image_path = os.path.join(root, filename)\n",
    "            print(f\"ğŸ” Processing: {image_path}\")\n",
    "            \n",
    "            ocr_results = extract_text_google_vision(image_path)\n",
    "\n",
    "            if not ocr_results:\n",
    "                print(\"âš  No text detected!\")\n",
    "            else:\n",
    "                for item in ocr_results:\n",
    "                    output_records.append({\n",
    "                        \"image\": filename,\n",
    "                        \"text\": item['text'],\n",
    "                        \"bbox\": item['bbox'],\n",
    "                        \"engine\": item['engine']\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbd964-68b5-4072-bfe8-b4cc4df40ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bf9d499-1322-43bd-b2ec-1f9b28bf9ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Processing: cheques\\TestSet\\X\\X_013.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_014.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_015.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_016.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_017.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_018.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_019.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_020.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_021.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_022.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_023.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_024.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_025.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_026.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_027.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_028.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_029.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_030.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_031.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_032.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_033.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_034.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_035.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_036.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_037.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_038.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_040.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_041.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\X_042.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\.ipynb_checkpoints\\X_013-checkpoint.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\.ipynb_checkpoints\\X_014-checkpoint.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\X\\.ipynb_checkpoints\\X_016-checkpoint.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_013.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_014.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_015.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_016.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_017.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_018.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_019.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_020.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_021.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_022.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_023.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_024.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_025.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_026.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_027.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_028.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_029.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_030.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_031.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_032.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_033.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_034.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_035.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_036.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_037.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_038.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_040.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_041.jpeg\n",
      "ğŸ” Processing: cheques\\TestSet\\y\\y_042.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_000.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_001.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_002.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_003.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_004.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_005.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_006.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_008.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_010.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_012.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_043.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_044.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_045.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_046.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_047.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_048.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_049.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_050.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_051.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_052.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_053.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_054.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_055.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_056.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_057.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_058.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_059.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_060.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_061.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_062.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_063.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_064.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_065.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_066.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_067.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_068.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_069.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_070.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_071.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_072.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_073.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_074.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_075.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_076.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_077.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_080.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_081.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_082.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_083.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_084.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_085.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_086.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_087.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_088.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_089.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_090.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_091.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_092.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_093.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_094.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_095.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_096.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_097.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_098.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_099.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_100.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_101.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_102.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_103.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_104.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_105.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_106.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_107.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_108.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_109.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_110.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_111.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_112.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_113.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_114.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_115.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_116.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_117.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_118.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_119.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_120.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_121.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_122.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_123.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_124.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_125.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_126.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_127.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_128.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_129.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_130.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_131.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_132.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_133.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_134.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_135.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_136.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_137.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_138.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_139.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_140.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_141.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_142.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_143.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_144.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_145.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_146.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_147.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_148.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_149.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_150.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_151.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_152.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_153.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_154.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_155.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_156.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_157.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_158.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_159.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_160.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_161.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_162.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\X\\X_163.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_000.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_001.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_002.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_003.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_004.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_005.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_006.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_008.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_010.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_012.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_043.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_044.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_045.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_046.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_047.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_048.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_049.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_050.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_051.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_052.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_053.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_054.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_055.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_056.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_057.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_058.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_059.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_060.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_061.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_062.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_063.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_064.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_065.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_066.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_067.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_068.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_069.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_070.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_071.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_072.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_073.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_074.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_075.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_076.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_077.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_080.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_081.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_082.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_083.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_084.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_085.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_086.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_087.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_088.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_089.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_090.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_091.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_092.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_093.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_094.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_095.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_096.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_097.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_098.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_099.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_100.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_101.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_102.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_103.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_104.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_105.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_106.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_107.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_108.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_109.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_110.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_111.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_112.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_113.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_114.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_115.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_116.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_117.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_118.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_119.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_120.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_121.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_122.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_123.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_124.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_125.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_126.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_127.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_128.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_129.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_130.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_131.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_132.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_133.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_134.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_135.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_136.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_137.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_138.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_139.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_140.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_141.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_142.jpeg\n",
      "âš  No text detected!\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_143.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_144.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_145.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_146.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_147.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_148.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_149.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_150.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_151.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_152.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_153.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_154.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_155.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_156.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_157.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_158.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_159.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_160.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_161.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_162.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\y_163.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\.ipynb_checkpoints\\y_000-checkpoint.jpeg\n",
      "ğŸ” Processing: cheques\\TrainSet\\y\\.ipynb_checkpoints\\y_001-checkpoint.jpeg\n",
      "âœ… OCR results saved to cheque_ocr_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ‘‡ Set your API key here again\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"VisionApiKey.json\"\n",
    "\n",
    "image_folder = \"cheques\"\n",
    "output_records = []\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "for root, dirs, files in os.walk(image_folder):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(valid_extensions):\n",
    "            image_path = os.path.join(root, filename)\n",
    "            print(f\"ğŸ” Processing: {image_path}\")\n",
    "            try:\n",
    "                ocr_results = extract_text_google_vision(image_path)\n",
    "\n",
    "                if not ocr_results:\n",
    "                    print(\"âš  No text detected!\")\n",
    "                else:\n",
    "                    for item in ocr_results:\n",
    "                        output_records.append({\n",
    "                            \"image\": filename,\n",
    "                            \"text\": item['text'],\n",
    "                            \"bbox\": item['bbox'],\n",
    "                            \"engine\": item['engine']\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error extracting text from {image_path}: {e}\")\n",
    "\n",
    "# Save output\n",
    "df = pd.DataFrame(output_records)\n",
    "if not df.empty:\n",
    "    df.to_excel(\"cheque_ocr_output.xlsx\", index=False)\n",
    "    print(\"âœ… OCR results saved to cheque_ocr_output.xlsx\")\n",
    "else:\n",
    "    print(\"âš  No OCR results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc4f702-5280-458c-aef6-74ce292c3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group text by image\n",
    "image_text_map = defaultdict(str)\n",
    "\n",
    "for record in output_records:\n",
    "    image = record['image']\n",
    "    text = record['text']\n",
    "    image_text_map[image] += \" \" + text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5aa2f16-b7f7-499d-8542-ff36ebdce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_fields(text_block):\n",
    "    return {\n",
    "        \"Loan Account Number\": re.search(r'\\b\\d{9,15}\\b', text_block),\n",
    "        \"IFSC Code\": re.search(r'[A-Z]{4}0[A-Z0-9]{6}', text_block),\n",
    "        \"MICR Code\": re.search(r'\\b\\d{9}\\b', text_block),\n",
    "        \"Installment Date\": re.search(r'\\b\\d{2}[-/]\\d{2}[-/]\\d{4}\\b', text_block),\n",
    "        \"Instrument Amount\": re.search(r'â‚¹?\\s?\\d{1,3}(,\\d{3})*(\\.\\d{2})?', text_block),\n",
    "        \"Account Name\": re.search(r'Pay to the Order of\\s+([A-Za-z ]+)', text_block),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b66e884-2faf-40d0-8bae-1b9a3569aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_structured_data = []\n",
    "\n",
    "for image, text in image_text_map.items():\n",
    "    fields = extract_fields(text)\n",
    "    structured = {\"image\": image}\n",
    "    for key, match in fields.items():\n",
    "        structured[key] = match.group(0) if match else None\n",
    "    extracted_structured_data.append(structured)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a650a75b-a659-436b-90e7-589e52633a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR output saved!\n"
     ]
    }
   ],
   "source": [
    "df_structured = pd.DataFrame(extracted_structured_data)\n",
    "df_structured.to_excel(\"cheque_final_output.xlsx\", index=False)\n",
    "print(\"Final OCR output saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fb597-388c-4c53-9375-f729398a32d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
